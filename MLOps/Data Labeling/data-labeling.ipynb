{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-10-04T14:46:48.038030Z","iopub.execute_input":"2023-10-04T14:46:48.038383Z","iopub.status.idle":"2023-10-04T14:46:48.397896Z","shell.execute_reply.started":"2023-10-04T14:46:48.038356Z","shell.execute_reply":"2023-10-04T14:46:48.396816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Week 3 - Ungraded Lab: Data Labeling\n\nWelcome to the ungraded lab for week 3 of Machine Learning Engineering for Production. In this lab, you will see how the data labeling process affects the performance of a classification model. Labeling data is usually a very labor intensive and costly task but it is of great importance.\n\nAs you saw in the lectures there are many ways to label data, this is dependant on the strategy used. Recall the example with the iguanas, all of the following are valid labeling alternatives but they clearly follow different criteria.\n\nYou can think of every labeling strategy as a result of different labelers following different labeling rules. If your data is labeled by people using different criteria this will have a negative impact on your learning algorithm. It is desired to have consistent labeling across your dataset.\n\nThis lab will touch on the effect of labeling strategies from a slighlty different angle. You will explore how different strategies affect the performance of a machine learning model by simulating the process of having different labelers label the data. This, by defining a set of rules and performing automatic labeling based on those rules.\n\nThe main objective of this ungraded lab is to compare performance across labeling options to understand the role that good labeling plays on the performance of Machine Learning models, these options are:\n\n1. Randomly generated labels (performance lower bound)\n2. Automatic generated labels based on three different label strategies\n3. True labels (performance upper bound)\n\nAlthough the example with the iguanas is a computer vision task, the same concepts regarding labeling can be applied to other types of data. In this lab you will be working with text data, concretely you will be using a dataset containing comments from the 2015 top 5 most popular Youtube videos. Each comment has been labeled as spam or not_spam depending on its contents.","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2023-10-04T14:46:48.399706Z","iopub.execute_input":"2023-10-04T14:46:48.400334Z","iopub.status.idle":"2023-10-04T14:46:48.403802Z","shell.execute_reply.started":"2023-10-04T14:46:48.400307Z","shell.execute_reply":"2023-10-04T14:46:48.402888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n## Loading the dataset\n\n\n","metadata":{}},{"cell_type":"code","source":"# import the data from the excel file\nxlsx_file_path = '/kaggle/input/youtube-spam-collection/youtubeSpamCollection.xlsx'\n# Specify the sheet name or index containing the CSV data within the Excel file\nsheet_name = 'Worksheet'\n# Read the Excel file\nxls = pd.ExcelFile(xlsx_file_path)\n# Parse the specific sheet containing the CSV data into a DataFrame\ndf = pd.read_excel(xls, sheet_name)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-10-04T14:46:48.405032Z","iopub.execute_input":"2023-10-04T14:46:48.405358Z","iopub.status.idle":"2023-10-04T14:46:48.930227Z","shell.execute_reply.started":"2023-10-04T14:46:48.405278Z","shell.execute_reply":"2023-10-04T14:46:48.929075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# look at the top tuples of this dataframe\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2023-10-04T14:46:48.931442Z","iopub.execute_input":"2023-10-04T14:46:48.931832Z","iopub.status.idle":"2023-10-04T14:46:48.955283Z","shell.execute_reply.started":"2023-10-04T14:46:48.931807Z","shell.execute_reply":"2023-10-04T14:46:48.954308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# now we rename the columns of this dataframe\ndf = df.rename(columns={\"CONTENT\": \"text\", \"CLASS\": \"label\"})","metadata":{"execution":{"iopub.status.busy":"2023-10-04T14:46:48.957292Z","iopub.execute_input":"2023-10-04T14:46:48.957569Z","iopub.status.idle":"2023-10-04T14:46:48.966009Z","shell.execute_reply.started":"2023-10-04T14:46:48.957546Z","shell.execute_reply":"2023-10-04T14:46:48.964767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set a seed for the order of rows\ndf = df.sample(frac=1, random_state=824)\ndf.reset_index()","metadata":{"execution":{"iopub.status.busy":"2023-10-04T14:46:48.967269Z","iopub.execute_input":"2023-10-04T14:46:48.967568Z","iopub.status.idle":"2023-10-04T14:46:48.995152Z","shell.execute_reply.started":"2023-10-04T14:46:48.967543Z","shell.execute_reply":"2023-10-04T14:46:48.994032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_labeled = df","metadata":{"execution":{"iopub.status.busy":"2023-10-04T14:46:48.996615Z","iopub.execute_input":"2023-10-04T14:46:48.997030Z","iopub.status.idle":"2023-10-04T14:46:49.012375Z","shell.execute_reply.started":"2023-10-04T14:46:48.996993Z","shell.execute_reply":"2023-10-04T14:46:49.011390Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To have a feeling of how the data is organized, let's inspect the top 5 rows of the data:","metadata":{}},{"cell_type":"code","source":"# Take a look at the first 5 rows\ndf_labeled.head()","metadata":{"execution":{"iopub.status.busy":"2023-10-04T14:46:49.013555Z","iopub.execute_input":"2023-10-04T14:46:49.013907Z","iopub.status.idle":"2023-10-04T14:46:49.031380Z","shell.execute_reply.started":"2023-10-04T14:46:49.013868Z","shell.execute_reply":"2023-10-04T14:46:49.030238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print actual value count\nprint(f\"Value counts for each class:\\n\\n{df_labeled.label.value_counts()}\\n\")\n\n# Display pie chart to visually check the proportion\ndf_labeled.label.value_counts().plot.pie(y='label', title='Proportion of each class')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-10-04T14:46:49.032308Z","iopub.execute_input":"2023-10-04T14:46:49.032642Z","iopub.status.idle":"2023-10-04T14:46:49.289206Z","shell.execute_reply.started":"2023-10-04T14:46:49.032615Z","shell.execute_reply":"2023-10-04T14:46:49.287771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#  Drop unused columns\ndf_labeled = df_labeled.drop(['COMMENT_ID', 'AUTHOR', 'DATE'], axis=1)\n\n# Look at the cleaned dataset\ndf_labeled.head()","metadata":{"execution":{"iopub.status.busy":"2023-10-04T14:53:53.938871Z","iopub.execute_input":"2023-10-04T14:53:53.939209Z","iopub.status.idle":"2023-10-04T14:53:53.950374Z","shell.execute_reply.started":"2023-10-04T14:53:53.939183Z","shell.execute_reply":"2023-10-04T14:53:53.949103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Splitting the dataset\n\nBefore jumping to the data labeling section let's split the data into training and test sets so you can use the latter to measure the performance of models that were trained using data labeled through different methods. As a safety measure when doing this split, remember to use stratification so the proportion of classes is maintained within each split.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Save the text into the X variable\nX = df_labeled.drop(\"label\", axis=1)\n\n# Save the true labels into the y variable\ny = df_labeled[\"label\"]\n\n# Use 1/5 of the data for testing later\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n\n# Print number of comments for each set\nprint(f\"There are {X_train.shape[0]} comments for training.\")\nprint(f\"There are {X_test.shape[0]} comments for testing\")","metadata":{"execution":{"iopub.status.busy":"2023-10-04T14:55:15.157941Z","iopub.execute_input":"2023-10-04T14:55:15.158285Z","iopub.status.idle":"2023-10-04T14:55:16.052047Z","shell.execute_reply.started":"2023-10-04T14:55:15.158252Z","shell.execute_reply":"2023-10-04T14:55:16.050933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n\nLet's do a visual to check that the stratification actually worked:\n","metadata":{}},{"cell_type":"code","source":"plt.subplot(1, 3, 1)\ny_train.value_counts().plot.pie(y='label', title='Proportion of each class for train set', figsize=(10, 6))\n\nplt.subplot(1, 3, 3)\ny_test.value_counts().plot.pie(y='label', title='Proportion of each class for test set', figsize=(10, 6))\n\nplt.tight_layout()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-10-04T14:55:35.264096Z","iopub.execute_input":"2023-10-04T14:55:35.264466Z","iopub.status.idle":"2023-10-04T14:55:35.570718Z","shell.execute_reply.started":"2023-10-04T14:55:35.264440Z","shell.execute_reply":"2023-10-04T14:55:35.569250Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n\nBoth, the training and test sets a balanced proportion of examples per class. So, the code successfully implemented stratification.\n\nLet's get going!\n","metadata":{}},{"cell_type":"markdown","source":"## Data Labeling\n### Establishing performance lower and upper bounds for reference\n\nTo properly compare different labeling strategies you need to establish a baseline for model accuracy, in this case you will establish both a lower and an upper bound to compare against.\n\n### Calculate accuracy of a labeling strategy\n\nCountVectorizer is a handy tool included in the sklearn ecosystem to encode text based data.\n\nFor more information on how to work with text data using sklearn check out [this resource](https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html).\n","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\n\n# Allow unigrams and bigrams\nvectorizer = CountVectorizer(ngram_range=(1, 5))","metadata":{"execution":{"iopub.status.busy":"2023-10-04T14:58:43.991194Z","iopub.execute_input":"2023-10-04T14:58:43.991585Z","iopub.status.idle":"2023-10-04T14:58:44.010103Z","shell.execute_reply.started":"2023-10-04T14:58:43.991555Z","shell.execute_reply":"2023-10-04T14:58:44.009188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now that the text encoding is defined, you need to select a model to make predictions. For simplicity you will use a Multinomial Naive Bayes classifier. This model is well suited for text classification and is fairly quick to train.\n\nLet's define a function which will handle the model fitting and print out the accuracy on the test data:","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nfrom sklearn.naive_bayes import MultinomialNB\n\n\ndef calculate_accuracy(X_tr, y_tr, X_te=X_test, y_te=y_test, \n                       clf=MultinomialNB(), vectorizer=vectorizer):\n    \n    # Encode train text\n    X_train_vect = vectorizer.fit_transform(X_tr.text.tolist())\n    \n    # Fit model\n    clf.fit(X=X_train_vect, y=y_tr)\n    \n    # Vectorize test text\n    X_test_vect = vectorizer.transform(X_te.text.tolist())\n    \n    # Make predictions for the test set\n    preds = clf.predict(X_test_vect)\n    \n    # Return accuracy score\n    return accuracy_score(preds, y_te)\n","metadata":{"execution":{"iopub.status.busy":"2023-10-04T14:59:41.402677Z","iopub.execute_input":"2023-10-04T14:59:41.403353Z","iopub.status.idle":"2023-10-04T14:59:41.419508Z","shell.execute_reply.started":"2023-10-04T14:59:41.403322Z","shell.execute_reply":"2023-10-04T14:59:41.418437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's create a dictionary to store the accuracy of each labeling method:","metadata":{}},{"cell_type":"code","source":"# Empty dictionary\naccs = dict()","metadata":{"execution":{"iopub.status.busy":"2023-10-04T15:00:02.417711Z","iopub.execute_input":"2023-10-04T15:00:02.418098Z","iopub.status.idle":"2023-10-04T15:00:02.422226Z","shell.execute_reply.started":"2023-10-04T15:00:02.418071Z","shell.execute_reply":"2023-10-04T15:00:02.421471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Random Labeling\n\nGenerating random labels is a natural way to establish a lower bound. You will expect that any successful alternative labeling model to outperform randomly generated labels.\n\nNow let's calculate the accuracy for the random labeling method (notice that, every time, the result is slightly different because the labels are assigned randomly, and thus their quality may be slightly better or worse, but they are always random, thus the model does not really learn anything).\n","metadata":{}},{"cell_type":"code","source":"# Calculate random labels\nrnd_labels = np.random.randint(0, 2, X_train.shape[0])\n\n# Feed them alongside X_train to calculate_accuracy function\nrnd_acc = calculate_accuracy(X_train, rnd_labels)\n\nrnd_acc","metadata":{"execution":{"iopub.status.busy":"2023-10-04T15:00:49.926575Z","iopub.execute_input":"2023-10-04T15:00:49.926968Z","iopub.status.idle":"2023-10-04T15:00:50.101079Z","shell.execute_reply.started":"2023-10-04T15:00:49.926940Z","shell.execute_reply":"2023-10-04T15:00:50.099956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n\nYou will see a different accuracy everytime you run the previous cell. This is due to the fact that the labeling is done randomly. Remember, this is a binary classification problem and both classes are balanced, so you can expect to see accuracies that revolve around 50%.\n\nTo further gain intuition let's look at the average accuracy over 10 runs:\n","metadata":{}},{"cell_type":"code","source":"# Empty list to save accuracies\nrnd_accs = []\n\nfor _ in range(10):\n    # Add every accuracy to the list\n    rnd_accs.append(calculate_accuracy(X_train, np.random.randint(0, 2, X_train.shape[0])))\n\n# Save result in accs dictionary\naccs['random-labels'] = sum(rnd_accs)/len(rnd_accs)\n\n# Print result\nprint(f\"The random labelling method achieved and accuracy of {accs['random-labels']*100:.2f}%\")","metadata":{"execution":{"iopub.status.busy":"2023-10-04T15:02:12.749768Z","iopub.execute_input":"2023-10-04T15:02:12.750170Z","iopub.status.idle":"2023-10-04T15:02:14.413691Z","shell.execute_reply.started":"2023-10-04T15:02:12.750143Z","shell.execute_reply":"2023-10-04T15:02:14.412671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Random labelling is completely disregarding the information from the solution space you are working on, and is just guessing the correct label. You can't probably do worse than this (or maybe you can). For this reason, this method serves as reference for comparing other labeling methods","metadata":{}},{"cell_type":"markdown","source":"### Labeling with true values\n\nNow let's look at the other end of the spectrum, this is using the correct labels for your data points. Let's retrain the Multinomial Naive Bayes classifier with the actual labels","metadata":{}},{"cell_type":"code","source":"# Calculate accuracy when using the true labels\ntrue_acc = calculate_accuracy(X_train, y_train)\n\n# Save the result\naccs['true-labels'] = true_acc\n\nprint(f\"The true labelling method achieved and accuracy of {accs['true-labels']*100:.2f}%\")\n\n","metadata":{"execution":{"iopub.status.busy":"2023-10-04T15:03:07.024482Z","iopub.execute_input":"2023-10-04T15:03:07.024855Z","iopub.status.idle":"2023-10-04T15:03:07.206308Z","shell.execute_reply.started":"2023-10-04T15:03:07.024811Z","shell.execute_reply":"2023-10-04T15:03:07.204941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Training with the true labels produced a noticeable boost in accuracy. This is expected as the classifier is now able to properly identify patterns in the training data which were lacking with randomly generated labels.\n\nAchieving higher accuracy is possible by either fine-tunning the model or even selecting a different one. For the time being you will keep the model as it is and use this accuracy as what we should strive for with the automatic labeling algorithms you will see next.","metadata":{}},{"cell_type":"markdown","source":"\n## Automatic labeling - Trying out different labeling strategies\n\nLet's suppose that for some reason you don't have access to the true labels associated with each data point in this dataset. It is a natural idea to think that there are patterns in the data that will provide clues of which are the correct labels. This is of course very dependant on the kind of data you are working with and to even hypothesize which patterns exist requires great domain knowledge.\n\nThe dataset used in this lab was used for this reason. It is reasonable for many people to come up with rules that might help identify a spam comment from a non-spam one for a Youtube video. In the following section you will be performing automatic labeling using such rules. **You can think of each iteration of this process as a labeler with different criteria for labeling** and your job is to hire the most promising one.\n\nNotice the word **rules**. In order to perform automatic labeling you will define some rules such as \"if the comment contains the word 'free' classify it as spam\".\n\nFirst things first. Let's define how we are going to encode the labeling:\n\n* `SPAM` is represented by 1\n* `NOT_SPAM` by 0\n* `NO_LABEL` as -1\n\nYou might be wondering about the `NO_LABEL` keyword. Depending on the rules you come up with, these might not be applicable to some data points. For such cases it is better to refuse from giving a label rather than guessing, which you already saw yields poor results.","metadata":{}},{"cell_type":"markdown","source":"\n### First iteration - Define some rules\n\nFor this first iteration you will create three rules based on the intuition of common patterns that appear on spam comments. The rules are simple, classify as SPAM if any of the following patterns is present within the comment or NO_LABEL otherwise:\n* `free` - spam comments usually lure users by promoting free stuff\n* `subs` - spam comments tend to ask users to subscribe to some website or channel\n* `http` - spam comments include links very frequently","metadata":{}},{"cell_type":"code","source":"def labeling_rules_1(x):\n    \n    # Convert text to lowercase\n    x = x.lower()\n    \n    # Define list of rules\n    rules = [\n        \"free\" in x,\n        \"subs\" in x,\n        \"http\" in x\n    ]\n    \n    # If the comment falls under any of the rules classify as SPAM\n    if any(rules):\n        return 1\n    \n    # Otherwise, NO_LABEL\n    return -1","metadata":{"execution":{"iopub.status.busy":"2023-10-04T15:07:07.380924Z","iopub.execute_input":"2023-10-04T15:07:07.381307Z","iopub.status.idle":"2023-10-04T15:07:07.387355Z","shell.execute_reply.started":"2023-10-04T15:07:07.381279Z","shell.execute_reply":"2023-10-04T15:07:07.386019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Apply the rules the comments in the train set\nlabels = [labeling_rules_1(label) for label in X_train.text]\n\n# Convert to a numpy array\nlabels = np.asarray(labels)\n\n# Take a look at the automatic labels\nlabels","metadata":{"execution":{"iopub.status.busy":"2023-10-04T15:07:09.307103Z","iopub.execute_input":"2023-10-04T15:07:09.307451Z","iopub.status.idle":"2023-10-04T15:07:09.317036Z","shell.execute_reply.started":"2023-10-04T15:07:09.307426Z","shell.execute_reply":"2023-10-04T15:07:09.315884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For lots of points the automatic labeling algorithm decided to not settle for a label, this is expected given the nature of the rules that were defined. These points should be deleted since they don't provide information about the classification process and tend to hurt performance.","metadata":{}},{"cell_type":"code","source":"# Create the automatic labeled version of X_train by removing points with NO_LABEL label\nX_train_al = X_train[labels != -1]\n\n# Remove predictions with NO_LABEL label\nlabels_al = labels[labels != -1]\n\nprint(f\"Predictions with concrete label have shape: {labels_al.shape}\")\n\nprint(f\"Proportion of data points kept: {labels_al.shape[0]/labels.shape[0]*100:.2f}%\")","metadata":{"execution":{"iopub.status.busy":"2023-10-04T15:07:55.260397Z","iopub.execute_input":"2023-10-04T15:07:55.260770Z","iopub.status.idle":"2023-10-04T15:07:55.267801Z","shell.execute_reply.started":"2023-10-04T15:07:55.260743Z","shell.execute_reply":"2023-10-04T15:07:55.266591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n\nNotice that only 379 data points remained out of the original 1564. The rules defined didn't provide enough context for the labeling algorithm to settle on a label, so around 75% of the data has been trimmed.\n\nLet's test the accuracy of the model when using these automatic generated labels:\n","metadata":{}},{"cell_type":"code","source":"# Compute accuracy when using these labels\niter_1_acc = calculate_accuracy(X_train_al, labels_al)\n\n# Display accuracy\nprint(f\"First iteration of automatic labeling has an accuracy of {iter_1_acc*100:.2f}%\")\n\n# Save the result\naccs['first-iteration'] = iter_1_acc","metadata":{"execution":{"iopub.status.busy":"2023-10-04T15:09:25.973054Z","iopub.execute_input":"2023-10-04T15:09:25.973424Z","iopub.status.idle":"2023-10-04T15:09:26.056635Z","shell.execute_reply.started":"2023-10-04T15:09:25.973396Z","shell.execute_reply":"2023-10-04T15:09:26.055424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's compare this accuracy to the baselines by plotting:","metadata":{}},{"cell_type":"code","source":"def plot_accuracies(accs=accs):\n    colors = list(\"rgbcmy\")\n    items_num = len(accs)\n    cont = 1\n\n    for x, y in accs.items():\n        if x in ['true-labels', 'random-labels', 'true-labels-best-clf']:\n            plt.hlines(y, 0, (items_num-2)*2, colors=colors.pop())\n        else:\n            plt.scatter(cont, y, s=100)\n            cont+=2\n    plt.legend(accs.keys(), loc=\"center left\",bbox_to_anchor=(1, 0.5))\n    plt.show()\n    \nplot_accuracies()","metadata":{"execution":{"iopub.status.busy":"2023-10-04T15:09:49.525575Z","iopub.execute_input":"2023-10-04T15:09:49.525941Z","iopub.status.idle":"2023-10-04T15:09:49.798274Z","shell.execute_reply.started":"2023-10-04T15:09:49.525915Z","shell.execute_reply":"2023-10-04T15:09:49.797287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This first iteration had an accuracy very close to the random labeling, we should strive to do better than this.\n\nBefore moving forward let's define the `label_given_rules` function that performs all of the steps you just saw, these are:\n* Apply the rules to a dataframe of comments\n* Cast the resulting labels to a numpy array\n* Delete all data points with NO_LABEL as label\n* Calculate the accuracy of the model using the automatic labels\n* Save the accuracy for plotting\n* Print some useful metrics of the process\n\n","metadata":{}},{"cell_type":"code","source":"def label_given_rules(df, rules_function, name, \n                      accs_dict=accs, verbose=True):\n    \n    # Apply labeling rules to the comments\n    labels = [rules_function(label) for label in df.text]\n    \n    # Convert to a numpy array\n    labels = np.asarray(labels)\n    \n    # Save initial number of data points\n    initial_size = labels.shape[0]\n    \n    # Trim points with NO_LABEL label\n    X_train_al = df[labels != -1]\n    labels = labels[labels != -1]\n    \n    # Save number of data points after trimming\n    final_size = labels.shape[0]\n    \n    # Compute accuracy\n    acc = calculate_accuracy(X_train_al, labels)\n    \n    # Print useful information\n    if verbose:\n        print(f\"Proportion of data points kept: {final_size/initial_size*100:.2f}%\\n\")\n        print(f\"{name} labeling has an accuracy of {acc*100:.2f}%\\n\")\n        \n    # Save accuracy to accuracies dictionary\n    accs_dict[name] = acc\n    \n    return X_train_al, labels, acc","metadata":{"execution":{"iopub.status.busy":"2023-10-04T15:13:25.169868Z","iopub.execute_input":"2023-10-04T15:13:25.170318Z","iopub.status.idle":"2023-10-04T15:13:25.178729Z","shell.execute_reply.started":"2023-10-04T15:13:25.170288Z","shell.execute_reply":"2023-10-04T15:13:25.177433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Going forward we should come up with rules that have a better coverage of the training data, thus making pattern discovery an easier task. Also notice how the rules were only able to label as either SPAM or NO_LABEL, we should also create some rules that help the identification of NOT_SPAM comments.\n\n## Second iteration - Coming up with better rules\n\nIf you inspect the comments in the dataset you might be able to distinguish certain patterns at a glimpse. For example, not spam comments often make references to either the number of views since these were the most watched videos of 2015 or the song in the video and its contents . As for spam comments other common patterns are to promote gifts or ask to follow some channel or website.\n\nLet's create some new rules that include these patterns:\n","metadata":{}},{"cell_type":"code","source":"def labeling_rules_2(x):\n    \n    # Convert text to lowercase\n    x = x.lower()\n    \n    # Define list of rules to classify as NOT_SPAM\n    not_spam_rules = [\n        \"view\" in x,\n        \"song\" in x\n    ]\n    \n    # Define list of rules to classify as SPAM\n    spam_rules = [\n        \"free\" in x,\n        \"subs\" in x,\n        \"gift\" in x,\n        \"follow\" in x,\n        \"http\" in x\n    ]\n    \n    # Classify depending on the rules\n    if any(not_spam_rules):\n        return 0\n    \n    if any(spam_rules):\n        return 1\n    \n    return -1","metadata":{"execution":{"iopub.status.busy":"2023-10-04T15:14:13.587881Z","iopub.execute_input":"2023-10-04T15:14:13.588264Z","iopub.status.idle":"2023-10-04T15:14:13.594944Z","shell.execute_reply.started":"2023-10-04T15:14:13.588234Z","shell.execute_reply":"2023-10-04T15:14:13.593577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This new set of rules looks more promising as it includes more patterns to classify as SPAM as well as some patterns to classify as NOT_SPAM. This should result in more data points with a label different to NO_LABEL.\n\nLet's check if this is the case.","metadata":{}},{"cell_type":"code","source":"label_given_rules(X_train, labeling_rules_2, \"second-iteration\")\n\nplot_accuracies()","metadata":{"execution":{"iopub.status.busy":"2023-10-04T15:14:42.728584Z","iopub.execute_input":"2023-10-04T15:14:42.728992Z","iopub.status.idle":"2023-10-04T15:14:43.129021Z","shell.execute_reply.started":"2023-10-04T15:14:42.728962Z","shell.execute_reply":"2023-10-04T15:14:43.127740Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n\nThis time 40% of the original dataset was given a decisive label and there were data points for both labels, this helped the model reach a higher accuracy when compared to the first iteration. Now the accuracy is considerably higher than the random labeling but it is still very far away from the upper bound.\n\nLet's see if we can make it even better!\n","metadata":{}},{"cell_type":"markdown","source":"\n### Third Iteration - Even more rules\n\nThe rules we have defined so far are doing a fair job. Let's add two additional rules, one for classifying SPAM comments and the other for the opposite task.\n\nAt a glimpse it looks like NOT_SPAM comments are usually shorter. This may be due to them not including hyperlinks but also in general they tend to be more concrete such as \"I love this song!\".\n\nLet's take a look at the average number of characters for SPAM comments vs NOT_SPAM oned:\n","metadata":{}},{"cell_type":"code","source":"from statistics import mean\n\nprint(f\"NOT_SPAM comments have an average of {mean([len(t) for t in df_labeled[df_labeled.label==0].text]):.2f} characters.\")\nprint(f\"SPAM comments have an average of {mean([len(t) for t in df_labeled[df_labeled.label==1].text]):.2f} characters.\")","metadata":{"execution":{"iopub.status.busy":"2023-10-04T15:15:52.997451Z","iopub.execute_input":"2023-10-04T15:15:52.997820Z","iopub.status.idle":"2023-10-04T15:15:53.011509Z","shell.execute_reply.started":"2023-10-04T15:15:52.997792Z","shell.execute_reply":"2023-10-04T15:15:53.010206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It sure looks like there is a big difference in the number of characters for both types of comments.\n\nTo decide on a threshold to classify as NOT_SPAM let's plot a histogram of the number of characters for NOT_SPAM comments:","metadata":{}},{"cell_type":"code","source":"plt.hist([len(t) for t in df_labeled[df_labeled.label==0].text], range=(0,100))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-10-04T15:16:16.197130Z","iopub.execute_input":"2023-10-04T15:16:16.197474Z","iopub.status.idle":"2023-10-04T15:16:16.431001Z","shell.execute_reply.started":"2023-10-04T15:16:16.197448Z","shell.execute_reply":"2023-10-04T15:16:16.429860Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The majority of NOT_SPAM comments have 30 or less characters so we'll use that as a threshold.\n\nAnother prevalent pattern in spam comments is to ask users to \"check out\" a channel, website or link.\n\nLet's add these two new rules:\n","metadata":{}},{"cell_type":"code","source":"def labeling_rules_3(x):\n    \n    # Convert text to lowercase\n    x = x.lower()\n    \n    # Define list of rules to classify as NOT_SPAM\n    not_spam_rules = [\n        \"view\" in x,\n        \"song\" in x,\n        len(x) < 30\n    ]\n    \n\n    # Define list of rules to classify as SPAM\n    spam_rules = [\n        \"free\" in x,\n        \"subs\" in x,\n        \"gift\" in x,\n        \"follow\" in x,\n        \"http\" in x,\n        \"check out\" in x\n    ]\n    \n    # Classify depending on the rules\n    if any(not_spam_rules):\n        return 0\n    \n    if any(spam_rules):\n        return 1\n    \n    return -1","metadata":{"execution":{"iopub.status.busy":"2023-10-04T15:16:56.164287Z","iopub.execute_input":"2023-10-04T15:16:56.164683Z","iopub.status.idle":"2023-10-04T15:16:56.171500Z","shell.execute_reply.started":"2023-10-04T15:16:56.164653Z","shell.execute_reply":"2023-10-04T15:16:56.170245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label_given_rules(X_train, labeling_rules_3, \"third-iteration\")\n\nplot_accuracies()","metadata":{"execution":{"iopub.status.busy":"2023-10-04T15:17:05.089233Z","iopub.execute_input":"2023-10-04T15:17:05.089582Z","iopub.status.idle":"2023-10-04T15:17:05.521286Z","shell.execute_reply.started":"2023-10-04T15:17:05.089556Z","shell.execute_reply":"2023-10-04T15:17:05.520185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"These new rules do a pretty good job at both, covering the dataset and having a good model accuracy. To be more concrete this labeling strategy reached an accuracy of ~86%! We are getting closer and closer to the upper bound defined by using the true labels.\n\nWe could keep going on adding more rules to improve accuracy and we do encourage you to try it out yourself!","metadata":{}},{"cell_type":"markdown","source":"### Come up with your own rules\n\nThe following cells contain some code to help you inspect the dataset for patterns and to test out these patterns. The ones used before are commented out in case you want start from scratch or re-use them.","metadata":{}},{"cell_type":"code","source":"# Configure pandas to print out all rows to check the complete dataset\npd.set_option('display.max_rows', None)\n\n# Check NOT_SPAM comments\ndf_labeled[df_labeled.label==0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def your_labeling_rules(x):\n    \n    # Convert text to lowercase\n    x = x.lower()\n    \n    # Define your rules for classifying as NOT_SPAM\n    not_spam_rules = [\n#         \"view\" in x,\n#         \"song\" in x,\n#         len(x) < 30\n    ]\n    \n\n    # Define your rules for classifying as SPAM\n    spam_rules = [\n#         \"free\" in x,\n#         \"subs\" in x,\n#         \"gift\" in x,\n#         \"follow\" in x,\n#         \"http\" in x,\n#         \"check out\" in x\n    ]\n    \n    # Classify depending on your rules\n    if any(not_spam_rules):\n        return 0\n    \n    if any(spam_rules):\n        return 1\n    \n    return -1\n\n\ntry:\n    label_given_rules(X_train, your_labeling_rules, \"your-iteration\")\n    plot_accuracies()\n    \nexcept ValueError:\n    print(\"You have not defined any rules.\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n\nCongratulations on finishing this ungraded lab!\n\nBy now you should have a better understanding of having good labelled data. In general, the better your labels are, the better your models will be. Also it is important to realize that the process of correctly labeling data is a very complex one. Remember, you can think of each one of the iterations of the automatic labeling process to be a different labeler with different criteria for labeling. If you assume you are hiring labelers you will want to hire the latter for sure!\n\nAnother important point to keep in mind is that establishing baselines to compare against is really important as they provide perspective on how well your data and models are performing.\n\nKeep it up!\n","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}